{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651bdf05",
   "metadata": {},
   "source": [
    "1. What is a Tensor?\n",
    "\n",
    "At its core, a tensor is a data structure used in PyTorch (and other deep learning frameworks) to store data. This data can be anything from a single number (a scalar, or 0-dimensional tensor), to a list of numbers (a vector, or 1-dimensional tensor), a table of numbers (a matrix, or 2-dimensional tensor), or even higher-dimensional arrays (like images, which are often 3D or 4D tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb2a63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a754298",
   "metadata": {},
   "source": [
    "Tensor represents a Mulidimensional array of numerical Values. In one dimensional case i.e when only one axis is needed for the data, a tensor is called a vector.\n",
    "\n",
    "With two axes, a tensor is called a matrix.\n",
    "and with k> 2 axes , we just call themm kth- order tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "311398a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from Python list:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Type: <class 'torch.Tensor'>\n",
      ", Shape: torch.Size([2, 2]), Dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "## Creating a Tensor from python list or Numpy Arrays\n",
    "\n",
    "data = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(f'Tensor from Python list:\\n{x_data}')\n",
    "print(f'Type: {type(x_data)}\\n, Shape: {x_data.shape}, Dtype: {x_data.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "883e36c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor from Numpy array:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Type: <class 'torch.Tensor'>\n",
      ", Shape: torch.Size([2, 2]), Dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "## numpy array from the same list:\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f'Tensor from Numpy array:\\n{x_np}')\n",
    "print(f'Type: {type(x_np)}\\n, Shape: {x_np.shape}, Dtype: {x_np.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b32b5fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of ones with same shape as x_data:\n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "Type: <class 'torch.Tensor'>\n",
      ", Shape: torch.Size([2, 2]), Dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 2.2. Creating Tensors with Specific Properties (Size, Data Type)\n",
    "\n",
    "x_ones = torch.ones_like(x_data)\n",
    "print(f'Tensor of ones with same shape as x_data:\\n{x_ones}')\n",
    "print(f'Type: {type(x_ones)}\\n, Shape: {x_ones.shape}, Dtype: {x_ones.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0064d389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of random values (like x_data, float):\n",
      "tensor([[0.3465, 0.8846],\n",
      "        [0.1156, 0.8616]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensor of random values (same shape)\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # Overrides the datatype to float\n",
    "print(f\"Tensor of random values (like x_data, float):\\n{x_rand}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7e98a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor of ones with shape (2, 3):\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Tensor of zeros with shape (2, 3):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Tensor of random values with shape (2, 3):\n",
      "tensor([[0.8408, 0.9410, 0.8698],\n",
      "        [0.9990, 0.3734, 0.8037]])\n"
     ]
    }
   ],
   "source": [
    "## Creating a tensor with specific shpae directly\n",
    "shape = (2, 3)  # Shape of the tensor\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "rand_tensor = torch.rand(shape)\n",
    "\n",
    "print(f'Tensor of ones with shape {shape}:\\n{ones_tensor}')\n",
    "print(f'Tensor of zeros with shape {shape}:\\n{zeros_tensor}')\n",
    "print(f'Tensor of random values with shape {shape}:\\n{rand_tensor}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae908194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([4, 4])\n",
      "Data type of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensors have useful attributes like shape, data type (dtype), and device.\n",
    "\n",
    "tensor = torch.ones(4, 4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Data type of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\\n\") # By default, it's 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a993613",
   "metadata": {},
   "source": [
    "Self-Check 1:\n",
    "\n",
    "Try creating a tensor of shape (3, 2) filled with all zeros.\n",
    "\n",
    "What's the difference between torch.tensor() and torch.ones_like()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80f9aac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of zeros tensor: torch.Size([3, 2])\n",
      "Data type of zeros tensor: torch.float32\n",
      "Device of zeros tensor: cpu\n",
      "\n",
      "Tensor of shape (3,2) with zeros:\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shapes_zeros= torch.zeros(3,2)\n",
    "print(f\"Shape of zeros tensor: {shapes_zeros.shape}\")\n",
    "print(f\"Data type of zeros tensor: {shapes_zeros.dtype}\")\n",
    "print(f\"Device of zeros tensor: {shapes_zeros.device}\\n\") # By\n",
    "print(f'Tensor of shape (3,2) with zeros:\\n{shapes_zeros}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19b45b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.tensor() creates a new tensor with the specified data, while\\ntorch.ones_like() creates a new tensor filled with ones that has the same shape and data type\\nas the input tensor.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Difference between torch.tensor() and torch.ones_like() is that \n",
    "'''\n",
    "torch.tensor() creates a new tensor with the specified data, while\n",
    "torch.ones_like() creates a new tensor filled with ones that has the same shape and data type\n",
    "as the input tensor.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad55beca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype= torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6ef33",
   "metadata": {},
   "source": [
    "Each of these values is called an element of the tensor.\n",
    "\n",
    "The tensor x contains 12 elements and we can inspect total no of elements in a tensor via its *numel* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4938fed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ddbd289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361ede11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reshaping the tnesor:\n",
    "X= x.reshape(3,4) ## 3,4 is possible because 3*4=12 and we had 12 elements in x\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4796e888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88776f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## supposw we don't know the number of rows, but we know the number of columns:\n",
    "X = x.reshape(-1, 4)  # -1 means \"infer this dimension\"\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eda41a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## It worked because 12 elements can be reshaped into 3 rows and 4 columns.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3433c4a",
   "metadata": {},
   "source": [
    "## Some times we need tensor to be initialized with all 0's or 1's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6897011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((3,2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a9e7bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((3,2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb64d350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8552,  0.3204,  0.9229,  0.4613],\n",
       "        [-1.8338,  0.4313, -0.5318, -0.8995],\n",
       "        [ 0.9290,  0.0437, -0.8232,  0.8999]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Often we wish to sample each element randomly from a given probability distribution.\n",
    "torch.randn(3,4) # Random numbers from a normal distribution with mean 0 and variance 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd0e0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can also create a tensors by supplying the exact values for each element:\n",
    "custom_tensor=torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])\n",
    "custom_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c4465c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48fba91",
   "metadata": {},
   "source": [
    "## Indexing and Slicing\n",
    "As with Python lists, we can access tensor elements by indexing (starting with 0). To access\n",
    "an element based on its position relative to the end of the list, we can use negative indexing.\n",
    "Finally, we can access whole ranges of indices via slicing (e.g., X[start:stop]), where\n",
    "the returned value includes the first index (start) but not the last (stop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73521cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "242b3841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c22ae612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "750d6b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5., 17.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## writing elements of matrix:\n",
    "X[1, 2] = 17\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34dfd83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2, :] = 12 ## all rows up to but not including row 2 and all the columns\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f5f2f",
   "metadata": {},
   "source": [
    "## Common Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51fb7302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tensor Operations ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Tensor Operations ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7fd8b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## example tensor to be created.\n",
    "tensor= torch.ones(4,4)\n",
    "print(f\"Original Tensor:\\n{tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb0c5bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise addition (tensor + 2):\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Arithmetic operations\n",
    "\n",
    "# Element-wise addition\n",
    "tensor_add = tensor +2\n",
    "print(f\"Element-wise addition (tensor + 2):\\n{tensor_add}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "338be3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise multiplication (tensor * 3):\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Element-wise multiplication\n",
    "tensor_mul = tensor * 3\n",
    "print(f\"Element-wise multiplication (tensor * 3):\\n{tensor_mul}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "737799c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar multiplication (tensor * 2):\n",
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Scalar Multiplication\n",
    "scalar_multiplication = tensor * 2\n",
    "print(f\"Scalar multiplication (tensor * 2):\\n{scalar_multiplication}\\n\")\n",
    "## same as element-wise multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d42f9",
   "metadata": {},
   "source": [
    "## Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac24e193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "Tensor B:\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For matrix multiplication, ensure dimensions are compatible.\n",
    "# (n, m) @ (m, p) -> (n, p)\n",
    "tensor_A = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor_B = torch.tensor([[5, 6], [7, 8]])\n",
    "print(f\"Tensor A:\\n{tensor_A}\\n\")\n",
    "print(f\"Tensor B:\\n{tensor_B}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb9ace0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Tensor A: torch.Size([2, 2])\n",
      "Shape of Tensor B: torch.Size([2, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check the shapes of the tensors before multiplication\n",
    "print(f\"Shape of Tensor A: {tensor_A.shape}\")\n",
    "print(f\"Shape of Tensor B: {tensor_B.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a304489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication (A @ B):\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## matches the condition for matrix multiplication i.e mxn @ nxp -> mxp\n",
    "tensor_mul_mat = torch.matmul(tensor_A, tensor_B)\n",
    "print(f\"Matrix multiplication (A @ B):\\n{tensor_mul_mat}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b31c2ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication (A @ B):\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor_mul_mat = tensor_A @ tensor_B  # Using the @ operator for matrix multiplication\n",
    "print(f\"Matrix multiplication (A @ B):\\n{tensor_mul_mat}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6e47fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor before in-place operation:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n",
      "Tensor after in-place addition (tensor.add_(2)):\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In-place operations (denoted by a trailing underscore '_')\n",
    "# These operations modify the tensor directly without creating a new one.\n",
    "\n",
    "print(f\"Original Tensor before in-place operation:\\n{tensor}\\n\")\n",
    "tensor.add_(2) # Adds 2 to each element in-place\n",
    "print(f\"Tensor after in-place addition (tensor.add_(2)):\\n{tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53750060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor for indexing:\n",
      "tensor([[0.6789, 0.2120, 0.8864, 0.9112],\n",
      "        [0.6824, 0.7148, 0.5188, 0.5017],\n",
      "        [0.0067, 0.6525, 0.9039, 0.4577],\n",
      "        [0.1572, 0.2909, 0.2554, 0.0124]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.2. Indexing and Slicing\n",
    "# Tensors can be indexed and sliced just like NumPy arrays or Python lists.\n",
    "\n",
    "tensor = torch.rand(4, 4)\n",
    "print(f\"Random Tensor for indexing:\\n{tensor}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03e9deb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the tensor: torch.Size([4, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape= tensor.shape\n",
    "print(f'Shape of the tensor: {shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7d8698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of the tensor:\n",
      "tensor([0.6789, 0.2120, 0.8864, 0.9112])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the first row of the tensor\n",
    "first_row = tensor[0]\n",
    "print(f\"First row of the tensor:\\n{first_row}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2602908b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last column of the tensor:\n",
      "tensor([0.9112, 0.5017, 0.4577, 0.0124])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Getting the last column of the tensor\n",
    "last_col = tensor[:, -1]\n",
    "print(f\"Last column of the tensor:\\n{last_col}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f812ddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element at row 1, column 2: 0.5188109874725342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Get a specific element ( e.g element at row 1, column 2)\n",
    "specific_element = tensor[1, 2]\n",
    "print(f\"Element at row 1, column 2: {specific_element}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ff46b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-tensor (first two rows, all columns from second onward):\n",
      "tensor([[0.2120, 0.8864, 0.9112],\n",
      "        [0.7148, 0.5188, 0.5017]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## slicing a sub-section\n",
    "sub_tensor = tensor[0:2, 1: ]\n",
    "print(f\"Sub-tensor (first two rows, all columns from second onward):\\n{sub_tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2209cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor (1D): tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
      "Reshaped to 3x3:\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.3. Reshaping Tensors\n",
    "# `view()` and `reshape()` are used to change the shape of a tensor.\n",
    "# `view()` requires the new tensor to share the same underlying data with the original.\n",
    "# `reshape()` can return a copy if the original tensor is not contiguous in memory.\n",
    "\n",
    "tensor = torch.arange(9) # Creates a tensor [0, 1, ..., 8]\n",
    "print(f\"Original tensor (1D): {tensor}\")\n",
    "\n",
    "# Reshape to a 3x3 matrix\n",
    "reshaped_tensor = tensor.view(3, 3) # Or `tensor.reshape(3, 3)`\n",
    "print(f\"Reshaped to 3x3:\\n{reshaped_tensor}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b397d732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened tensor: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flattening a tensor (e.g., for input to a fully connected layer)\n",
    "flattened_tensor = reshaped_tensor.view(-1) # -1 infers the dimension\n",
    "print(f\"Flattened tensor: {flattened_tensor}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e962c758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened tensor using flatten(): tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattened_tensor = reshaped_tensor.flatten()  # Another way to flatten\n",
    "print(f\"Flattened tensor using flatten(): {flattened_tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "20828d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed tensor:\n",
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transposing a tensor (swapping dimensions)\n",
    "transposed_tensor = reshaped_tensor.T # Also `reshaped_tensor.transpose(0, 1)`\n",
    "print(f\"Transposed tensor:\\n{transposed_tensor}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f2fe0",
   "metadata": {},
   "source": [
    "## Self-Check 2:\n",
    "\n",
    "1. Create two 2\n",
    "*times2* tensors, A and B, with values of your choice.\n",
    "2. Perform element-wise multiplication on A and B.\n",
    "3. Perform matrix multiplication on A and B.\n",
    "4. Given a tensor `t = torch.arange(12).reshape(3, 4)`, how would you select the second row and the third column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db3168df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A with specific size and dtype:\n",
      "tensor([[ 1.7439, -1.0478],\n",
      "        [ 1.3917, -0.9843]])\n",
      "\n",
      "Tensor B with specific size and dtype:\n",
      "tensor([[-0.3332, -0.1754],\n",
      "        [ 0.2403,  0.8403]], dtype=torch.float64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Creating a Tensor with a specific size and data type\n",
    "tensor_A= torch.randn(2,2, dtype= torch.float32)  # Random tensor with float32 type\n",
    "print(f\"Tensor A with specific size and dtype:\\n{tensor_A}\\n\")\n",
    "tensor_B = torch.randn(2, 2, dtype=torch.float64)  # Random tensor with float64 type\n",
    "print(f\"Tensor B with specific size and dtype:\\n{tensor_B}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93376932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise multiplication of Tensor A and B:\n",
      "tensor([[-0.5811,  0.1838],\n",
      "        [ 0.3344, -0.8271]], dtype=torch.float64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Element-wise multiplication of tensor A and B\n",
    "## Note: The tensors must have the same shape for element-wise operations.\n",
    "elementwise_mul = tensor_A * tensor_B\n",
    "print(f\"Element-wise multiplication of Tensor A and B:\\n{elementwise_mul}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3674e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication of Tensor A and B:\n",
      "tensor([[-0.8329, -1.1864],\n",
      "        [-0.7003, -1.0713]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Matrix multiplication of tensor A and B\n",
    "matrix_mul = torch.matmul(tensor_A, tensor_B.to(tensor_A.dtype))  # Ensure dtype compatibility\n",
    "print(f\"Matrix multiplication of Tensor A and B:\\n{matrix_mul}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f279990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor t:\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Given a tensor `t = torch.arange(12).reshape(3, 4)`, how would you select the second row and the third column?\n",
    "t = torch.arange(12).reshape(3, 4)\n",
    "print(f\"Original tensor t:\\n{t}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b727bdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected element (second row, third column): 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## selecting second row and the third column.\n",
    "\n",
    "selected= t[1, 2]  # Indexing starts at 0, so this is the second row and third column\n",
    "print(f\"Selected element (second row, third column): {selected}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537f718",
   "metadata": {},
   "source": [
    "## 4. Device Management (CPU vs. GPU)\n",
    "\n",
    "One of the most powerful features of PyTorch is its ability to seamlessly move computations to a GPU (Graphics Processing Unit) when available. GPUs are highly parallel processors, making them ideal for the massive parallel computations required in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2cc6d794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Device Management ---\n",
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Device Management ---\")\n",
    "# Tensors can be moved between CPU and GPU for faster computation.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') # Use GPU if available\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2e8e9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor on CPU: cpu\n"
     ]
    }
   ],
   "source": [
    "# Move a tensor to the specified device\n",
    "tensor_on_cpu = torch.tensor([1, 2, 3])\n",
    "print(f\"Tensor on CPU: {tensor_on_cpu.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eec7ee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor on cpu: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor_on_device = tensor_on_cpu.to(device)\n",
    "print(f\"Tensor on {device}: {tensor_on_device.device}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "781362bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, operations remain on CPU.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Operations between tensors must be on the same device\n",
    "tensor_A_gpu = torch.rand(2, 2).to(device)\n",
    "tensor_B_gpu = torch.rand(2, 2).to(device)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    result_gpu = tensor_A_gpu + tensor_B_gpu\n",
    "    print(f\"Result of operation on GPU:\\n{result_gpu}\\n\")\n",
    "else:\n",
    "    print(\"GPU not available, operations remain on CPU.\\n\")\n",
    "result_gpu= tensor_A_gpu + tensor_B_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f6055bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result moved back to CPU:\n",
      "tensor([[1.4838, 0.6265],\n",
      "        [0.1455, 0.7708]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Moving back to CPU (useful for operations that require CPU, like converting to NumPy)\n",
    "result_cpu = result_gpu.to(\"cpu\")\n",
    "print(f\"Result moved back to CPU:\\n{result_cpu}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "58a02e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array from tensor:\n",
      "[[1.4837785  0.626454  ]\n",
      " [0.14545876 0.77075356]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting a PyTorch tensor to a NumPy array (requires tensor to be on CPU)\n",
    "np_array_from_tensor = result_cpu.numpy()\n",
    "print(f\"NumPy array from tensor:\\n{np_array_from_tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ad5af151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bfc37039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 2.7183e+00, 7.3891e+00, 2.0086e+01, 5.4598e+01, 1.4841e+02,\n",
       "        4.0343e+02, 1.0966e+03, 2.9810e+03, 8.1031e+03, 2.2026e+04, 5.9874e+04])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "192bc526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Element-wise operations on tensors\n",
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a39be880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ae2f0a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 1., 4., 3.],\n",
       "        [1., 2., 3., 4.],\n",
       "        [4., 3., 2., 1.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3abcc57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((X, Y), dim =0), torch.cat((X, Y), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb93863a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y  # Element-wise comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b09b8555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2b4642",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "By now, you know how to perform elementwise binary operations on two tensors of the\n",
    "same shape. Under certain conditions, even when shapes differ, we can still perform ele-\n",
    "mentwise binary operations by invoking the broadcasting mechanism. Broadcasting works\n",
    "according to the following two-step procedure: \n",
    "\n",
    "(i) expand one or both arrays by copying\n",
    "elements along axes with length 1 so that after this transformation, the two tensors have the\n",
    "same shape; \n",
    "\n",
    "(ii) perform an elementwise operation on the resulting arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f9d097c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d775ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b # Broadcasting: a is broadcasted to shape (3, 2) and b to shape (3, 2) by repeating their values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf80c6",
   "metadata": {},
   "source": [
    "## Saving Memory\n",
    "\n",
    "Running operations can cause new memory to be allocated to host results. For example, if\n",
    "we write `Y = X + Y`, we dereference the tensor that Yused to point to and instead point Yat\n",
    "the newly allocated memory. We can demonstrate this issue with Python’s id() function,\n",
    "which gives us the exact address of the referenced object in memory. Note that after we\n",
    "run `Y = Y + X`, `id(Y)`points to a different location. That is because Python first evaluates\n",
    "`Y + X`, allocating new memory for the result and then points Y to this new location in\n",
    "memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2c2afd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID before operation: 5351399952, ID after operation: 5351400672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X  # This creates a new tensor and assigns it to Y\n",
    "after = id(Y)\n",
    "print(f\"ID before operation: {before}, ID after operation: {after}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "472c1db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before == after  # This will be False, as Y now points to a new tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d09ad",
   "metadata": {},
   "source": [
    "This might be undesirable for two reasons. First, we do not want to run around allocat-\n",
    "ing memory unnecessarily all the time. In machine learning, we often have hundreds of\n",
    "megabytes of parameters and update all of them multiple times per second. Whenever\n",
    "possible, we want to perform these updates in place. Second, we might point at the same\n",
    "parameters from multiple variables. If we do not update in place, we must be careful to\n",
    "update all of these references, lest we spring a memory leak or inadvertently refer to stale\n",
    "parameters.\n",
    "\n",
    "Fortunately, performing in-place operations is easy. We can assign the result of an oper-\n",
    "ation to a previously allocated array Y by using slice notation: `Y[:] = <expression>`.\n",
    "To illustrate this concept, we overwrite the values of tensor Z, after initializing it, using\n",
    "zeros_like, to have the same shape as Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5b93ae13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 5351542048\n",
      "id(Z): 5351542048\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "22268b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290581c6",
   "metadata": {},
   "source": [
    "## Conversion to Other Python Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d62b3815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.from_numpy(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "baf6caf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82665b06",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ce475",
   "metadata": {},
   "source": [
    "## Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fbf2a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(os.path.join('..', 'data'), exist_ok = True)\n",
    "data_file = os.path.join('..', 'data', 'housing_tiny.csv')\n",
    "\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('''NumRooms,RoofType,Price\n",
    "NA,NA,127500\n",
    "2,NA,106000\n",
    "4,Slate,178100\n",
    "NA,NA,140000''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "36d779e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumRooms</th>\n",
       "      <th>RoofType</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Slate</td>\n",
       "      <td>178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumRooms RoofType   Price\n",
       "0       NaN      NaN  127500\n",
       "1       2.0      NaN  106000\n",
       "2       4.0    Slate  178100\n",
       "3       NaN      NaN  140000"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(data_file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e26c8a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  RoofType_Slate  RoofType_nan\n",
      "0       NaN           False          True\n",
      "1       2.0           False          True\n",
      "2       4.0            True         False\n",
      "3       NaN           False          True\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs= pd.get_dummies(inputs, dummy_na = True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e0a0fd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  RoofType_Slate  RoofType_nan\n",
      "0       3.0           False          True\n",
      "1       2.0           False          True\n",
      "2       4.0            True         False\n",
      "3       3.0           False          True\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "927e06d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 0., 1.],\n",
       "         [2., 0., 1.],\n",
       "         [4., 1., 0.],\n",
       "         [3., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500., 106000., 178100., 140000.], dtype=torch.float64))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.tensor(inputs.to_numpy(dtype=float))\n",
    "y = torch.tensor(targets.to_numpy(dtype=float))\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b9bb1812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 2-dimensional tensors as heatmap visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tensor = torch.rand(10, 10)  # Random tensor for demonstration\n",
    "plt.imshow(tensor, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Heatmap of Tensor Values')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Rows')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7580ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
